{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Import libraries needed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "XHsZCkEJ_2rU"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tqdm import tqdm\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import (Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout,\n",
        "                                     LSTM, TimeDistributed, GlobalAveragePooling2D)\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import kagglehub"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Download the latest version of WLASL dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X9gWN1rV_ve_",
        "outputId": "52a7857b-3dc3-49df-a913-5e63751ebef4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/risangbaskoro/wlasl-processed/versions/5\n"
          ]
        }
      ],
      "source": [
        "# Download latest version\n",
        "data_dir = kagglehub.dataset_download(\"risangbaskoro/wlasl-processed\")\n",
        "\n",
        "print(\"Path to dataset files:\", data_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "lOaXEyqOEVhf"
      },
      "outputs": [],
      "source": [
        "# Define the path to the dataset\n",
        "videos_dir = os.path.join(data_dir, 'videos') # path to the videos\n",
        "processed_data_dir = os.path.join(data_dir, 'processed_data') # path to the processed data\n",
        "\n",
        "if not os.path.exists(processed_data_dir): # create the directory if it does not exist\n",
        "    os.makedirs(processed_data_dir) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "bxMFmNYHEX4i"
      },
      "outputs": [],
      "source": [
        "# Helper functions\n",
        "def missing_videos():\n",
        "    missing_videos_file = os.path.join(data_dir, 'missing.txt')\n",
        "    missing_videos = set() # store the missing videos in a set so that we can easily check for membership\n",
        "    with open(missing_videos_file, 'r') as f: # read the file line by line\n",
        "        for line in f:\n",
        "            line = line.strip()   # remove leading and trailing whitespaces\n",
        "            if line: # if the line is not empty\n",
        "                missing_videos.add(line)\n",
        "    return missing_videos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "hdbkpT79EdX6"
      },
      "outputs": [],
      "source": [
        "def data_loader(): # load the data\n",
        "    WLASL_file = os.path.join(data_dir, 'WLASL_v0.3.json')\n",
        "    with open(WLASL_file, 'r') as f: # open the file\n",
        "        data = json.load(f) # load the data\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "FSKzOtqLEmuo"
      },
      "outputs": [],
      "source": [
        "missing = missing_videos() # get the missing videos\n",
        "data = data_loader() # load the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "3k235I85Ep3w"
      },
      "outputs": [],
      "source": [
        "# Load nslt_100 (list of video_ids to include)\n",
        "nslt_100_dir = os.path.join(data_dir, 'nslt_100.json') # path to the nslt_100 file\n",
        "with open(nslt_100_dir, 'r') as f:\n",
        "    nslt_100 = json.load(f) # load the nslt_100 file\n",
        "\n",
        "nslt_100 = list(nslt_100)[:50] # get the first 50 video_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "OPRh5Z0JErgu"
      },
      "outputs": [],
      "source": [
        "def filter_dataset(data, nslt_100): # filter the dataset\n",
        "    filtered_glosses = [] # store the filtered glosses\n",
        "    filtered_data = [] # store the filtered data\n",
        "    for gloss in data: \n",
        "        valid_instances = [] \n",
        "        for instance in gloss['instances']:     # iterate through the instances\n",
        "            if instance['video_id'] in nslt_100 and instance['video_id'] not in missing: # check if the video_id is in nslt_100 and not in missing\n",
        "                valid_instances.append(instance) # add the instance to the valid_instances list\n",
        "        if len(valid_instances) > 0: # if there are valid instances\n",
        "            filtered_glosses.append({'gloss': gloss['gloss'], 'instances': valid_instances}) # add the gloss and the valid instances to the filtered_glosses list\n",
        "            filtered_data.extend(valid_instances)\n",
        "    return filtered_data, filtered_glosses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "yHIh7BPPEyPn"
      },
      "outputs": [],
      "source": [
        "filtered_data, glosses = filter_dataset(data, nslt_100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S692H6uBE1cj",
        "outputId": "490490f4-02f8-4f53-bdeb-707887d4131c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of words in the filtered dataset: 15\n",
            "Number of videos in the filtered dataset: 35\n",
            "Example gloss: {'gloss': 'who', 'instances': [{'bbox': [165, 4, 472, 370], 'fps': 25, 'frame_end': -1, 'frame_start': 1, 'instance_id': 14, 'signer_id': 88, 'source': 'aslsignbank', 'split': 'train', 'url': 'https://aslsignbank.haskins.yale.edu/dictionary/protected_media/glossvideo/ASL/WH/WHO-1430.mp4', 'variation_id': 0, 'video_id': '66778'}, {'bbox': [167, 3, 471, 370], 'fps': 25, 'frame_end': -1, 'frame_start': 1, 'instance_id': 18, 'signer_id': 88, 'source': 'aslsignbank', 'split': 'train', 'url': 'https://aslsignbank.haskins.yale.edu/dictionary/protected_media/glossvideo/ASL/WH/WHO-2236.mp4', 'variation_id': 0, 'video_id': '66779'}]}\n"
          ]
        }
      ],
      "source": [
        "print(\"Number of words in the filtered dataset:\", len(glosses))\n",
        "print(\"Number of videos in the filtered dataset:\", len(filtered_data))\n",
        "print(\"Example gloss:\", glosses[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "U51lUHoGE3A1"
      },
      "outputs": [],
      "source": [
        "# Video processing function\n",
        "def process_video_fixed(video_file, frame_start, frame_end, bbox, output_dir, num_frames=16): # process the video\n",
        "    cap = cv2.VideoCapture(video_file) # open the video file\n",
        "    if not cap.isOpened(): # check if the video file is opened\n",
        "        print(f'Failed to open video file {video_file}') # print an error message\n",
        "        return # return None\n",
        "\n",
        "    frame_start = frame_start - 1 # subtract 1 from the frame_start assuming that the frame_start is 1-indexed\n",
        "    if frame_end == -1: # if frame_end is -1\n",
        "        frame_end = int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) - 1 # set frame_end to the total number of frames in the video file\n",
        "    else:\n",
        "        frame_end = frame_end - 1 # subtract 1 from the frame_end assuming that the frame_end is 1-indexed\n",
        "\n",
        "    x1, y1, x2, y2 = map(int, bbox) # map the bbox to integers\n",
        "\n",
        "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)) # get the width of the video\n",
        "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)) # get the height of the video\n",
        "\n",
        "    # Clamp bbox coords\n",
        "    x1 = max(0, min(x1, width - 1))\n",
        "    x2 = max(0, min(x2, width - 1))\n",
        "    y1 = max(0, min(y1, height - 1))\n",
        "    y2 = max(0, min(y2, height - 1))\n",
        "\n",
        "    cap.set(cv2.CAP_PROP_POS_FRAMES, frame_start) # set the frame position to frame_start\n",
        "    total_frames = frame_end - frame_start + 1 # calculate the total number of frames\n",
        "    step = total_frames // num_frames if total_frames >= num_frames else 1 # calculate the step size if the total_frames is greater than or equal to num_frames, else set the step size to 1\n",
        "\n",
        "    frames_selected = [] # store the selected frames\n",
        "    for idx in range(frame_start, frame_end + 1, step): \n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, idx) # set the frame position to idx\n",
        "        ret, frame = cap.read() # read the frame\n",
        "        if not ret: # if the frame is not read\n",
        "            break\n",
        "        crop_frame = frame[y1:y2, x1:x2] # crop the frame\n",
        "        gray_frame = cv2.cvtColor(crop_frame, cv2.COLOR_BGR2GRAY) # convert the frame to grayscale\n",
        "        resize_frame = cv2.resize(gray_frame, (224, 224)) # resize the frame to 224x224\n",
        "        frames_selected.append(resize_frame) # add the resized frame to the frames_selected list \n",
        "        if len(frames_selected) == num_frames: # if the length of frames_selected is equal to num_frames    \n",
        "            break   \n",
        "\n",
        "    # If fewer than num_frames, pad with the last frame\n",
        "    if len(frames_selected) < num_frames and len(frames_selected) > 0: # if the length of frames_selected is less than num_frames and greater than 0\n",
        "        last_frame = frames_selected[-1] # get the last frame\n",
        "        while len(frames_selected) < num_frames: # while the length of frames_selected is less than num_frames\n",
        "            frames_selected.append(last_frame) # add the last frame to the frames_selected list\n",
        "\n",
        "    # Save frames\n",
        "    for i, f in enumerate(frames_selected):\n",
        "        cv2.imwrite(os.path.join(output_dir, f'frame_{i:04d}.jpg'), f) # save the frame\n",
        "\n",
        "    cap.release()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "3FcAMP6PE9Qt"
      },
      "outputs": [],
      "source": [
        "def process_gloss(gloss_word, instances): # process the gloss\n",
        "    for instance in instances: # iterate through the instances\n",
        "        video_id = instance['video_id'] # get the video_id\n",
        "        if video_id in missing: # if the video_id is in missing\n",
        "            continue # continue to the next iteration\n",
        "        video_file = os.path.join(videos_dir, video_id + '.mp4') # get the video file\n",
        "        if not os.path.exists(video_file): # if the video file does not exist\n",
        "            continue # continue to the next iteration \n",
        "        split = instance['split'] # get the split\n",
        "        output_dir = os.path.join(processed_data_dir, split, gloss_word, video_id) # set the output directory path as the split/gloss_word/video_id\n",
        "        if not os.path.exists(output_dir): # if the output directory does not exist\n",
        "            os.makedirs(output_dir) # create the output directory\n",
        "        process_video_fixed(video_file, instance['frame_start'], instance['frame_end'], instance['bbox'], output_dir) # process the video"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "huY3FCEVFBrT",
        "outputId": "6f6db65d-e424-47fd-e0ae-f9ece525f9a0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing glosses: 100%|██████████| 15/15 [00:33<00:00,  2.21s/it]\n"
          ]
        }
      ],
      "source": [
        "# Process each gloss (You can comment out this loop if already processed)\n",
        "for gloss_data in tqdm(glosses, desc='Processing glosses'): # iterate through the glosses\n",
        "    gloss_word = gloss_data['gloss'] # get the gloss word\n",
        "    instances = gloss_data['instances'] # get the instances\n",
        "    process_gloss(gloss_word, instances) # process the gloss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "rwRG250dFJyO"
      },
      "outputs": [],
      "source": [
        "# Load processed data\n",
        "def load_data():\n",
        "    data_map = {} # store the data\n",
        "    splits = ['train', 'test', 'val'] # define the splits\n",
        "    for split in splits: # iterate through the splits\n",
        "        split_dir = os.path.join(processed_data_dir, split) # set the split directory path\n",
        "        if not os.path.exists(split_dir): # if the split directory does not exist\n",
        "            continue # continue to the next iteration\n",
        "        for gloss_word in os.listdir(split_dir): # iterate through the gloss words\n",
        "            gloss_dir = os.path.join(split_dir, gloss_word) # set the gloss directory path\n",
        "            if not os.path.isdir(gloss_dir):    # if the gloss directory does not exist\n",
        "                continue\n",
        "            for video_id in os.listdir(gloss_dir): # iterate through the video_ids\n",
        "                video_dir = os.path.join(gloss_dir, video_id) # set the video directory path by joining the gloss_dir and video_id\n",
        "                if os.path.isdir(video_dir): # if the video directory exists\n",
        "                    frame_files = sorted(os.listdir(video_dir)) # sort the frame files to maintain the order\n",
        "                    frame_paths = [os.path.join(video_dir, ff) for ff in frame_files] # get the frame paths by joining the video_dir and frame_files\n",
        "                    data_map[(gloss_word, video_id, split)] = frame_paths\n",
        "    return data_map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "dmlg_1J-FKoY"
      },
      "outputs": [],
      "source": [
        "data_map = load_data() # load the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "ZYKtNd49FMRk"
      },
      "outputs": [],
      "source": [
        "# Extract train/val/test sets\n",
        "X_train, Y_train = [], [] \n",
        "X_val, Y_val = [], []\n",
        "X_test, Y_test = [], []\n",
        "\n",
        "for (gloss_word, video_id, split) in data_map: # iterate through the data_map\n",
        "    frames = data_map[(gloss_word, video_id, split)] # get the frames from the data_map\n",
        "    if len(frames) < 16: # if the length of frames is less than 16\n",
        "        continue  # Skip if not enough frames after processing\n",
        "    # Load frames\n",
        "    clip = [] # store the frames\n",
        "    for f in frames: # iterate through the frames\n",
        "        img = cv2.imread(f, cv2.IMREAD_GRAYSCALE) # read the frame in grayscale\n",
        "        img = cv2.resize(img, (224, 224)) # resize the frame to 224x224\n",
        "        clip.append(img) # add the frame to the clip\n",
        "    clip = np.stack(clip, axis=0)  # shape: (num_frames, 224, 224)\n",
        "    # Keep only first 16 frames if more\n",
        "    clip = clip[:16] # keep only the first 16 frames    \n",
        "\n",
        "    if split == 'train': # if the split is train`\n",
        "        X_train.append(clip) # add the clip to X_train\n",
        "        Y_train.append(gloss_word) # add the gloss_word to Y_train\n",
        "    elif split == 'val': # if the split is val\n",
        "        X_val.append(clip) # add the clip to X_val\n",
        "        Y_val.append(gloss_word) # add the gloss_word to Y_val\n",
        "    elif split == 'test': # if the split is test\n",
        "        X_test.append(clip) # add the clip to X_test\n",
        "        Y_test.append(gloss_word) # add the gloss_word to Y_test\n",
        "\n",
        "X_train = np.array(X_train)  # (num_samples, 16, 224, 224)\n",
        "X_val = np.array(X_val)\n",
        "X_test = np.array(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "6BtVoRAMFZZd"
      },
      "outputs": [],
      "source": [
        "# Expand dims for channel\n",
        "X_train = np.expand_dims(X_train, -1)  # (num_samples, 16, 224, 224, 1)\n",
        "X_val = np.expand_dims(X_val, -1) # (num_samples, 16, 224, 224, 1)\n",
        "X_test = np.expand_dims(X_test, -1) # (num_samples, 16, 224, 224, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "66dMrDvKFm09"
      },
      "outputs": [],
      "source": [
        "# Normalize\n",
        "X_train = X_train / 255.0\n",
        "X_val = X_val / 255.0\n",
        "X_test = X_test / 255.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8MdSIg9CFnVz",
        "outputId": "0ae41809-4dba-4745-ff37-6316c9f787e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train shape: (24, 16, 224, 224, 1)\n",
            "Val shape: (7, 16, 224, 224, 1)\n",
            "Test shape: (4, 16, 224, 224, 1)\n"
          ]
        }
      ],
      "source": [
        "print(\"Train shape:\", X_train.shape)\n",
        "print(\"Val shape:\", X_val.shape)\n",
        "print(\"Test shape:\", X_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "--TGK5BSFqMh"
      },
      "outputs": [],
      "source": [
        "# Combine all labels before fitting\n",
        "all_labels = Y_train + Y_val + Y_test # Combine all labels\n",
        "\n",
        "label_encoder = LabelEncoder() # Initialize the label encoder\n",
        "label_encoder.fit(all_labels)  # Fit on all the labels\n",
        "\n",
        "Y_train_encoded = label_encoder.transform(Y_train) # Transform the Y_train labels\n",
        "Y_val_encoded = label_encoder.transform(Y_val) # Transform the Y_val labels \n",
        "Y_test_encoded = label_encoder.transform(Y_test) # Transform the Y_test labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "FFb6HoXOFsNF"
      },
      "outputs": [],
      "source": [
        "num_classes = len(label_encoder.classes_) # Get the number of classes from the label encoder by getting the length of the classes\n",
        "Y_train_one_hot = to_categorical(Y_train_encoded, num_classes) # One-hot encode the Y_train labels\n",
        "Y_val_one_hot = to_categorical(Y_val_encoded, num_classes) # One-hot encode the Y_val labels\n",
        "Y_test_one_hot = to_categorical(Y_test_encoded, num_classes) # One-hot encode the Y_test labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-27v7GGEFs8m",
        "outputId": "4a239f2b-91bc-45be-e723-d25b94f99c68"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of classes: 15\n"
          ]
        }
      ],
      "source": [
        "print(\"Number of classes:\", num_classes) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "3_437d5iFu9b"
      },
      "outputs": [],
      "source": [
        "# CNN feature extractor\n",
        "cnn_input = Input(shape=(224,224,1)) # Define the input shape\n",
        "x = Conv2D(32, (3,3), activation='relu')(cnn_input) # Add a Conv2D layer with 32 filters and relu activation\n",
        "x = MaxPooling2D((2,2))(x) # Add a MaxPooling2D layer\n",
        "x = Conv2D(64, (3,3), activation='relu')(x) # Add another Conv2D layer with 64 filters and relu activation\n",
        "x = MaxPooling2D((2,2))(x) # Add another MaxPooling2D layer\n",
        "x = Conv2D(128, (3,3), activation='relu')(x) # Add another Conv2D layer with 128 filters and relu activation\n",
        "x = MaxPooling2D((2,2))(x) # Add another MaxPooling2D layer\n",
        "x = GlobalAveragePooling2D()(x)  # Get a feature vector per frame\n",
        "cnn_model = Model(cnn_input, x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "qMzTy2gxF3nK"
      },
      "outputs": [],
      "source": [
        "# Now wrap it with TimeDistributed for the sequence of 16 frames\n",
        "sequence_input = Input(shape=(16, 224, 224, 1)) # Define the input shape\n",
        "td = TimeDistributed(cnn_model)(sequence_input) # shape: (batch, 16, feature_dim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "n2hsqOa7F8HB"
      },
      "outputs": [],
      "source": [
        "# LSTM to model temporal data\n",
        "lstm_out = LSTM(128)(td) # Add an LSTM layer with 128 units\n",
        "output = Dense(num_classes, activation='softmax')(lstm_out) # Add a Dense layer with softmax activation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "ZLsPCAQ-F_VA"
      },
      "outputs": [],
      "source": [
        "model = Model(sequence_input, output) # Define the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 257
        },
        "id": "oYuGp3PQGBAn",
        "outputId": "ab269fe4-5989-4161-897d-ea9cdc0032ee"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)     │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ time_distributed (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)             │          <span style=\"color: #00af00; text-decoration-color: #00af00\">92,672</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │         <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,935</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_1 (\u001b[38;5;33mInputLayer\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m1\u001b[0m)     │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ time_distributed (\u001b[38;5;33mTimeDistributed\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)             │          \u001b[38;5;34m92,672\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │         \u001b[38;5;34m131,584\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m)                  │           \u001b[38;5;34m1,935\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">226,191</span> (883.56 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m226,191\u001b[0m (883.56 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">226,191</span> (883.56 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m226,191\u001b[0m (883.56 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy']) # Compile the model with adam optimizer, categorical_crossentropy loss and accuracy metric\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YSA6AMCbGDtC",
        "outputId": "63ed6008-d297-4f01-9d36-384bf21a4f9d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 14s/step - accuracy: 0.0208 - loss: 2.6970 - val_accuracy: 0.1429 - val_loss: 2.6666\n",
            "Epoch 2/20\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 13s/step - accuracy: 0.2396 - loss: 2.5643 - val_accuracy: 0.1429 - val_loss: 2.6623\n",
            "Epoch 3/20\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 14s/step - accuracy: 0.1458 - loss: 2.3630 - val_accuracy: 0.1429 - val_loss: 3.1240\n",
            "Epoch 4/20\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 12s/step - accuracy: 0.2604 - loss: 2.2657 - val_accuracy: 0.1429 - val_loss: 3.4849\n",
            "Epoch 5/20\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 13s/step - accuracy: 0.0677 - loss: 2.2048 - val_accuracy: 0.1429 - val_loss: 3.6319\n",
            "Epoch 6/20\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 13s/step - accuracy: 0.1927 - loss: 2.2596 - val_accuracy: 0.1429 - val_loss: 3.6713\n",
            "Epoch 7/20\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 13s/step - accuracy: 0.1823 - loss: 2.1767 - val_accuracy: 0.1429 - val_loss: 3.6877\n",
            "Epoch 8/20\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 13s/step - accuracy: 0.1615 - loss: 2.1102 - val_accuracy: 0.1429 - val_loss: 3.6971\n",
            "Epoch 9/20\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 13s/step - accuracy: 0.2135 - loss: 2.0819 - val_accuracy: 0.2857 - val_loss: 3.7287\n",
            "Epoch 10/20\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 13s/step - accuracy: 0.1927 - loss: 2.1431 - val_accuracy: 0.1429 - val_loss: 3.8320\n",
            "Epoch 11/20\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 13s/step - accuracy: 0.3021 - loss: 2.0272 - val_accuracy: 0.1429 - val_loss: 3.8682\n",
            "Epoch 12/20\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 13s/step - accuracy: 0.3333 - loss: 2.0611 - val_accuracy: 0.1429 - val_loss: 3.9382\n",
            "Epoch 13/20\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 13s/step - accuracy: 0.2812 - loss: 1.9480 - val_accuracy: 0.1429 - val_loss: 3.8655\n",
            "Epoch 14/20\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 14s/step - accuracy: 0.3281 - loss: 1.9746 - val_accuracy: 0.1429 - val_loss: 3.9237\n",
            "Epoch 15/20\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 12s/step - accuracy: 0.3281 - loss: 1.8031 - val_accuracy: 0.2857 - val_loss: 3.9361\n",
            "Epoch 16/20\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 12s/step - accuracy: 0.2135 - loss: 1.8881 - val_accuracy: 0.1429 - val_loss: 4.1801\n",
            "Epoch 17/20\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 13s/step - accuracy: 0.2865 - loss: 1.7395 - val_accuracy: 0.2857 - val_loss: 4.1237\n",
            "Epoch 18/20\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 14s/step - accuracy: 0.1927 - loss: 1.8469 - val_accuracy: 0.1429 - val_loss: 4.0872\n",
            "Epoch 19/20\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 14s/step - accuracy: 0.3073 - loss: 1.7338 - val_accuracy: 0.0000e+00 - val_loss: 4.4300\n",
            "Epoch 20/20\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 13s/step - accuracy: 0.2917 - loss: 1.8473 - val_accuracy: 0.1429 - val_loss: 4.1298\n"
          ]
        }
      ],
      "source": [
        "# Train the model\n",
        "history = model.fit(\n",
        "    X_train, Y_train_one_hot,\n",
        "    validation_data=(X_val, Y_val_one_hot), \n",
        "    epochs=20,\n",
        "    batch_size=8\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XE_TmDhPGFKv",
        "outputId": "c9337f94-e3c0-4973-8914-24c7bd853ecf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy: 0.25\n"
          ]
        }
      ],
      "source": [
        "# Evaluate on test set\n",
        "test_loss, test_acc = model.evaluate(X_test, Y_test_one_hot, verbose=0)\n",
        "print(\"Test Accuracy:\", test_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xORFSK1fyCys",
        "outputId": "82bf0d2c-d23a-4914-8526-5bd651a50def"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final training accuracy: 0.375\n"
          ]
        }
      ],
      "source": [
        "train_loss, train_accuracy = model.evaluate(X_train, Y_train_one_hot, verbose=0)\n",
        "print(\"Final training accuracy:\", train_accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BFnihYU5yQdc",
        "outputId": "ce091d91-1f54-4f75-e38c-467326ac9e6a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final val accuracy: 0.1428571492433548\n"
          ]
        }
      ],
      "source": [
        "val_loss, val_accuracy = model.evaluate(X_val, Y_val_one_hot, verbose=0)\n",
        "print(\"Final val accuracy:\", val_accuracy)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
